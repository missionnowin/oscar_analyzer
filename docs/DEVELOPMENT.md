# Development & Architecture Guide

For extending and maintaining this codebase.

---

## Architecture Overview

```
Analysis Pipeline Architecture
│
├── INPUT LAYER
│   └── Oscar files (.f19)
│       ├── Oscar1992A format
│       └── OSC1997A (final_id_p_x) format
│
├── PARSING LAYER (utils/readers.py)
│   ├── OscarFormatDetector (auto-detect format)
│   ├── Oscar1992AReader (format-specific parser)
│   ├── OSC1997AReader (format-specific parser)
│   └── OscarReader (universal interface)
│
├── DATA MODEL LAYER (models/)
│   ├── Particle (kinematic state)
│   └── CumulativeSignature (detected effects)
│
├── ANALYSIS LAYER (analyzers/)
│   ├── AggregateAnalyzer (single-dataset stats)
│   ├── CumulativeAnalyzer (effect detection)
│   └── ComparisonAnalyzer (cross-dataset plots)
│
├── ORCHESTRATION LAYER (run_all_analysis.py)
│   └── UnifiedAnalysisEngine (parallel pipeline)
│       ├── Process files in parallel
│       ├── Coordinate aggregation
│       ├── Detect collision system
│       └── Generate outputs
│
└── OUTPUT LAYER
    ├── PNG plots (matplotlib)
    └── JSON statistics
        ├── stats/run_N_stats.json
        └── cumulative/run_N_cumulative.json
```

---

## Module Dependencies

```
run_all_analysis.py (main entry)
    ├─ UnifiedAnalysisEngine
    │   ├─ OscarReader (utils.readers)
    │   ├─ AggregateAnalyzer (analyzers.general)
    │   ├─ CumulativeAnalyzer (analyzers.cumulative)
    │   ├─ ComparisonAnalyzer (analyzers.general)
    │   ├─ CollisionSystemDetector (utils.collision_system_detector)
    │   └─ ProgressDisplay (utils.progress_display)
    │
    └─ _analyze_single_run (worker function)
        └─ [same as above]

Particle (models.particle)
    └─ @dataclass
        ├─ @property pt
        └─ @property eta

CumulativeSignature (models.cumulative_signature)
    └─ @dataclass

OscarReader (utils.readers)
    ├─ OscarFormatDetector.detect_format()
    ├─ Oscar1992AReader.stream_batch()
    ├─ OSC1997AReader.stream_batch()
    └─ Generator interface

AggregateAnalyzer (analyzers.general.aggregate_analyzer)
    ├─ accumulate_event()
    ├─ get_statistics()
    └─ plot_distributions()

CumulativeAnalyzer (analyzers.cumulative.cumulative_analyzer)
    ├─ process_batch()
    ├─ get_signatures()
    ├─ get_statistics()
    └─ plot_distributions()

ComparisonAnalyzer (analyzers.general.comparison_analyzer)
    ├─ plot_pt_broadening_ratio()
    ├─ plot_mean_pt_shift()
    ├─ plot_distribution_tails()
    ├─ plot_rapidity_eta_spectra()
    └─ generate_all_comparisons()
```

---

## Data Flow

### Single Run Processing

```
File Pair (modified.f19, unmodified.f19)
    ↓
[OscarReader] → Detect Format → Stream Batches
    ├─ Batch 1: [Event1, Event2, ...]
    ├─ Batch 2: [Event3, Event4, ...]
    └─ Batch N: [EventN-1, EventN]
    ↓
[Parallel Processing]
    ├─ Thread 1: Modified → AggregateAnalyzer → stats_mod
    ├─ Thread 2: Unmodified → AggregateAnalyzer → stats_unm
    └─ Thread 3: Both batches → CumulativeAnalyzer → signatures
    ↓
[Analysis]
    ├─ AggregateAnalyzer.plot_distributions() → PNG (modified/)
    ├─ AggregateAnalyzer.plot_distributions() → PNG (unmodified/)
    ├─ CumulativeAnalyzer.plot_distributions() → PNG (cumulative/)
    └─ ComparisonAnalyzer.generate_all_comparisons() → PNG (comparisons/)
    ↓
[JSON Output]
    ├─ stats/run_N_stats.json
    └─ cumulative/run_N_cumulative.json
```

### Batch Processing Flow

```
For batch in reader.stream_batch(500):
    │
    ├─ For event in batch:
    │   ├─ For particle in event:
    │   │   ├─ Compute pt = √(px² + py²)
    │   │   ├─ Compute eta = 0.5 × ln((p+pz)/(p-pz))
    │   │   └─ Pass to analyzers
    │   │
    │   ├─ AggregateAnalyzer.accumulate_event(event)
    │   │   ├─ Update multiplicity histogram
    │   │   ├─ Update pT statistics
    │   │   ├─ Update rapidity statistics
    │   │   └─ Update pseudorapidity statistics
    │   │
    │   └─ CumulativeAnalyzer.process_batch(batch_mod, batch_unm)
    │       ├─ Compare statistics
    │       ├─ Detect anomalies
    │       └─ Create signatures
    │
    └─ Clear batch from memory (gc.collect())
```

---

## Key Design Decisions

### 1. Streaming vs. Batch Loading

**Choice**: Stream batches (configurable size)

**Why**:
- Memory-efficient (process millions of events on modest hardware)
- Allows early termination (test on subset)
- Cache-friendly (keeps data hot)

**Alternative rejected**: Load entire file
- Would require 10+ GB RAM for typical RHIC datasets
- Infeasible on laptop/shared clusters

### 2. Parallel Processing

**Choice**: ProcessPoolExecutor (not threading)

**Why**:
- Python GIL means threads won't parallelize CPU work
- Multiple processes allow true parallelism
- Good I/O behavior (read from different file handles in parallel)

**Trade-off**:
- Inter-process communication has overhead
- Works well for this I/O-bound workload

### 3. Format Auto-Detection

**Choice**: Detect format by reading file header

**Why**:
- Oscar format can vary between generators (SMASH, UrQMD, etc.)
- Different versions have different column orders
- Graceful fallback if detection fails

**How**: Read first 3 lines, check for format signatures:
```python
if 'OSC1997A' in first_lines or 'final_id_p_x' in first_lines:
    format = OSC1997A
elif 'OSCAR' in first_lines.upper():
    format = OSCAR1992A
else:
    format = OSCAR1992A  # default
```

### 4. Collision System Detection

**Choice**: Heuristic-based from particle distributions

**Why**:
- Don't assume input metadata
- Works with any generator
- Provides confidence score

**Physics basis**:
- Average multiplicity → nucleus mass (A)
- Maximum pz → collision energy
- Charge distribution → nucleus identity (Z)

### 5. Live Progress Display

**Choice**: Cursor-based in-place update

**Why**:
- Shows real-time progress without log spam
- Works with multiprocessing (needs lock)
- User sees completion percentage

**Implementation**: 
- Manager locks to synchronize terminal access
- ANSI escape codes to move cursor
- One line per worker, updated in-place

### 6. JSON Output

**Choice**: Simple flat structure, easy parsing

**Why**:
- Language-agnostic
- Can be read by downstream analysis tools
- Version-stable format

**Not chosen**: HDF5
- Would require extra dependency (h5py)
- Unnecessary for 1D statistics

---

## Extension Points

### Adding a New Analyzer

**Example**: Angular correlation analyzer

```python
# File: analyzers/correlations/angle_correlator.py

from models.particle import Particle
from typing import List, Dict
import numpy as np

class AngleCorrelator:
    """Analyze angular correlations between particle pairs."""
    
    def __init__(self):
        self.correlations = []
    
    def process_event(self, particles: List[Particle]) -> None:
        """Analyze one event for correlations."""
        for i, p1 in enumerate(particles):
            for p2 in particles[i+1:]:
                # Compute azimuthal angle between particles
                dphi = np.arctan2(p2.py, p2.px) - np.arctan2(p1.py, p1.px)
                dphi = (dphi + np.pi) % (2*np.pi) - np.pi
                self.correlations.append(abs(dphi))
    
    def get_statistics(self) -> Dict:
        """Return angular correlation statistics."""
        return {
            'mean_dphi': np.mean(self.correlations),
            'std_dphi': np.std(self.correlations),
            'count': len(self.correlations)
        }
```

Then integrate into pipeline:

```python
# In run_all_analysis.py

from analyzers.correlations.angle_correlator import AngleCorrelator

# Inside _process_files_parallel():
correlator = AngleCorrelator()

for batch_mod, batch_unm in zip(...):
    for event in batch_mod:
        correlator.process_event(event)
```

### Adding a New Physics Observable

**Example**: K/π ratio calculation

```python
# File: analyzers/yields/particle_yield_analyzer.py

from collections import defaultdict
import numpy as np

class ParticleYieldAnalyzer:
    """Track abundance ratios (K/π, p/π, etc.)"""
    
    # PDG IDs for common particles
    PDG_IDS = {
        'pion': 211,      # π±
        'kaon': 321,      # K±
        'proton': 2212,
        'neutron': 2112
    }
    
    def __init__(self):
        self.yields = defaultdict(int)
    
    def accumulate_event(self, particles: List[Particle]) -> None:
        for p in particles:
            particle_id = abs(p.particle_id)
            if particle_id == self.PDG_IDS['pion']:
                self.yields['pion'] += 1
            elif particle_id == self.PDG_IDS['kaon']:
                self.yields['kaon'] += 1
            elif particle_id == self.PDG_IDS['proton']:
                self.yields['proton'] += 1
    
    def get_ratios(self) -> Dict[str, float]:
        """Return particle ratios."""
        total = sum(self.yields.values())
        if total == 0:
            return {}
        
        return {
            'K/pi': self.yields['kaon'] / (self.yields['pion'] + 1e-6),
            'p/pi': self.yields['proton'] / (self.yields['pion'] + 1e-6),
            'pion_fraction': self.yields['pion'] / total
        }
```

### Supporting a New Oscar Format

**Example**: HepMC3 format support

```python
# File: utils/readers.py - add new class

class HepMC3Reader:
    """Read HepMC3 format (XML-based)."""
    
    def __init__(self, filepath: str):
        self.filepath = filepath
    
    def stream_batch(self, batch_size: int):
        """Parse HepMC3 and yield batches."""
        import xml.etree.ElementTree as ET
        
        tree = ET.parse(self.filepath)
        root = tree.getroot()
        
        batch = []
        for event_elem in root.findall('event'):
            particles = []
            for vertex in event_elem.findall('vertex'):
                for particle in vertex.findall('particle'):
                    # Parse XML to Particle object
                    p = Particle(
                        particle_id=int(particle.get('pdgid')),
                        px=float(particle.find('momentum').get('px')),
                        # ... etc
                    )
                    particles.append(p)
            
            batch.append(particles)
            if len(batch) >= batch_size:
                yield batch
                batch = []
        
        if batch:
            yield batch

# Update OscarReader to auto-detect HepMC3:
if '.xml' in filepath.lower():
    return HepMC3Reader(filepath)
```

---

## Testing

### Unit Tests (Recommended Structure)

```
tests/
├── test_particle.py
│   ├── test_pt_calculation()
│   └── test_eta_edge_cases()
├── test_readers.py
│   ├── test_oscar1992a_parsing()
│   ├── test_osc1997a_parsing()
│   └── test_format_detection()
├── test_analyzers.py
│   ├── test_aggregate_stats()
│   ├── test_cumulative_detection()
│   └── test_comparison_plots()
└── test_integration.py
    └── test_full_pipeline()
```

### Example Test

```python
# test_particle.py

from models.particle import Particle
import math

def test_pt_calculation():
    """Verify transverse momentum calculation."""
    p = Particle(
        particle_id=211,
        px=3.0, py=4.0, pz=10.0,
        mass=0.14, x=0, y=0, z=0,
        E=10.5, t_formed=1.0, charge=1
    )
    
    expected_pt = math.sqrt(3.0**2 + 4.0**2)
    assert abs(p.pt - expected_pt) < 1e-6
    assert p.pt == 5.0

def test_eta_collinear():
    """Verify eta returns None for collinear particles."""
    p = Particle(
        particle_id=2212,
        px=0, py=0, pz=10.0,  # pure forward
        mass=0.938, x=0, y=0, z=0,
        E=10.1, t_formed=1.0, charge=1
    )
    
    # Collinear → eta is undefined
    assert p.eta is None or math.isnan(p.eta)
```

---

## Performance Optimization

### Current Bottlenecks

1. **File I/O** (~60% of time)
   - Reading large Oscar files sequentially
   - Parsing text lines
   
2. **Matplotlib plotting** (~30%)
   - Generating PNG files
   - Figure rendering

3. **Statistics computation** (~10%)
   - Histogram accumulation
   - Statistical calculations

### Optimization Opportunities

**Short-term** (easy):
- Use larger batch sizes (if memory allows)
- Pre-allocate numpy arrays
- Cache file positions for seeking

**Medium-term** (moderate):
- Multi-threaded file reading (I/O bound)
- Vectorized numpy operations
- Cython-compiled inner loops

**Long-term** (significant):
- C++ file parser (if needed)
- GPU-accelerated statistics (overkill for current size)
- HDF5 for faster I/O

### Profiling

```bash
# Profile single run
python3 -m cProfile -s cumulative run_all_analysis.py > profile.txt

# Memory profiling
pip install memory_profiler
python3 -m memory_profiler run_all_analysis.py
```

---

## Version Control Strategy

### Recommended Workflow

```
main branch (production)
    ↑
    │── hotfix/bug-fix (urgent production fixes)
    │
develop branch (integration)
    ↑
    ├── feature/new-analyzer
    ├── feature/new-format-support
    ├── fix/bug-description
    └── perf/optimization
```

### Commit Convention

```
[TYPE] Brief description

Longer explanation if needed.

Fixes #issue_number

TYPE: feature|fix|perf|docs|test|refactor
```

Example:
```
[feature] Add K/π ratio calculation

New ParticleYieldAnalyzer tracks abundance ratios
Useful for QCD phase diagram studies
```

---

## Documentation Maintenance

### When Updating Code

1. **Add docstrings** (NumPy format):
   ```python
   def calculate_pT_broadening(mod_stats: Dict, unm_stats: Dict) -> float:
       """
       Calculate transverse momentum broadening ratio.
       
       Parameters
       ----------
       mod_stats : Dict
           Statistics from modified dataset with key 'pt'
       unm_stats : Dict
           Statistics from unmodified dataset
       
       Returns
       -------
       float
           Ratio σ_pT(modified) / σ_pT(unmodified)
       
       Notes
       -----
       Ratio > 1.05 indicates significant multiple scattering.
       """
   ```

2. **Update API_REFERENCE.md** with new classes/methods

3. **Update README.md** if major changes to interface

4. **Add examples** to QUICKSTART.md if new feature

---

## Common Tasks

### Run subset for testing
```python
engine.run_parallel(runs=[1, 2, 3])  # Just 3 runs
```

### Profile memory usage
```bash
python3 -c "
from run_all_analysis import UnifiedAnalysisEngine
import tracemalloc

tracemalloc.start()
engine = UnifiedAnalysisEngine(...)
engine.run_parallel([1])
current, peak = tracemalloc.get_traced_memory()
print(f'Peak memory: {peak / 1024**2:.1f} MB')
"
```

### Generate minimal test data
```python
# Create toy Oscar file for testing
with open('test.f19', 'w') as f:
    for event_num in range(10):
        f.write(f"{event_num} 50\n")  # event header
        for i in range(50):
            f.write(f"{i} 0.14 0 0 0 1 0 0 1.1 0 1 0\n")
```

### Extract statistics quickly
```bash
python3 -c "
import json, sys
from pathlib import Path

for f in Path('results/stats').glob('*.json'):
    with open(f) as file:
        d = json.load(file)
    ratio = d['modified']['pt']['std'] / d['unmodified']['pt']['std']
    print(f'{f.stem}: {ratio:.4f}')
" | sort -t: -k2 -nr  # Sort by ratio descending
```

---

## Troubleshooting Development

### Import errors
```python
# Add project root to path:
import sys
sys.path.insert(0, '/path/to/project')
```

### Matplotlib backend issues
```python
import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend
```

### Multiprocessing won't start
```python
if __name__ == '__main__':
    # Always wrap main code
    engine = UnifiedAnalysisEngine(...)
    engine.run_parallel([1, 2, 3])
```

---

## Performance Benchmarks

On Intel i7-8700K (6 cores), 16GB RAM:

| Task | Time | Memory |
|------|------|--------|
| Parse 1000-event file | 0.5s | 50 MB |
| Analyze (modified+unmodified) | 2.0s | 300 MB |
| Generate 4 plots | 1.5s | 200 MB |
| JSON output | 0.1s | 50 MB |
| **Per run total** | ~4s | ~600 MB |
| **10 runs parallel (8 workers)** | ~10s | ~2 GB |

---

## Resources

- Python data science: https://numpy.org/, https://matplotlib.org/
- Oscar format: https://github.com/smash-transport/smash/wiki/Oscar-Format
- Multiprocessing: https://docs.python.org/3/library/multiprocessing.html
- Matplotlib animation: https://matplotlib.org/stable/api/animation_api.html
